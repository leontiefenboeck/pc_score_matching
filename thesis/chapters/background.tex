\renewcommand{\vec}[1]{\textbf{#1}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}


\chapter{Background}
\label{cha:background}

\section{Probabilistic Modelling}
\label{sec:pm}

At it's very core Machine Learning (ML) aims to create algorithms/programs that learn from data in order to reason about it, make future
predictions or perform all kinds of different inference tasks. One possible way to achieve this, especially when there is an inherent 
uncertainty in the data, which is the case for most real-world scenarios, is Probabilistic Modelling. In Probabilistic Modelling we 
use probabilities to express this uncertainty and the rules of probability theory for inference. \\
To make this more clear let's say we have some data and assume that this data was drawn randomly from some unknown distribution. 
We would call this distribution $P^*$. If this distribution was known to us all inference tasks would boil down to applying the basic 
rules of probability theory. The sum rule to compute marginals, the product rule to compute conditionals and more complex rules, derived 
from these two, to perform harder inference tasks. \\
However since we don't know $P^*$, we have to do something different, which leads us to a basic definition of probabilistic modelling:
Through machine learning methods we create a framework that approximates an original unknown distribution, from which we only have limited samples, 
as closely as possible in order to reason about it with the rules of probability theory. \cite{pc_intro}. \\

Before continuing with different approaches to probabilistic modelling, I want to introduce two key concepts that are often 
used to discuss these different methods. \\

Definition 1: Expressiveness - a probabilistic model is called expressive if the learned distribution approximates the original distribution to a high degree.
For instance this can be measured with Kullback-Leibler (KL) divergence \cite{kl_divergence}, which measures how similar two probability distributions are. 
Two identical distributions would then have a KL-divergence of $0$. In the Probabilistic Modelling case a KL-divergence of 0 would mean that the model is most exxpressive. \\

Definition 2: Tractability - a probabilistic model is called tractable with respect to an inference task, if the model can complete this 
task exactly and efficiently. Exactly in this case means without relying on approximation e.g. Monte-Carlo Simulation and efficiently 
means in linear time with respect to the model size. \\

In recent years expressive capability has increased considerably through models based on neural networks like Generative-Adversarial
Networks (GANs) \cite{gan}, Variational Autoencoder (VAE) \cite{vae} and many others. However what these models excel in for instance generating 
very realistic images or compelling text in language modelling, they lack in tractability for all except the most simplest inference tasks. 
\cite{pc_intro} \\

On the other hand there are many mostly older, less complex models like Gaussian Mixture Models (GMMs), Hidden Markos Models (HMMs) and so on, 
that lack in expressiveness and only work well enough for very simple data but can compute most if not all inference tasks tractably, which directly
leads to Probabilistic Circuits.  


\section{Probabilistic Circuits}
\label{sec:pc}

Recent progress in probabilistic modelling through .. have pushed the expressive capability (how close the 
approximated distribution is to the true one) ever forward. However what expressive gains these models achieve through 
their complexity they loose in tractability, the ability to perform inference tasks exactly and efficiently.
More concretely tractability in one inference task means that a model can perform the task exactly, without approximation, and in polynomial time with respct to the model size.
Except for the simplest taks, these models must rely on costly approximation.

On the other hand older approaches like .. which are less complex and therefore lack in expressiveness can perform 
all sort of inference tasks tractably.

Probabilistic Circuits are a framework for probabilistic models that try to be both expressive and tractable. 
In essence they try to achieve this by only introducing complexity in a controlled manner. To be valid a PC must 
adhere to certain structural constraints. 

\section{Score Matching}
\label{sec:sm}

\section{Sliced Score Matching}
\label{sec:ssm}

Why score matching instead of minimizing NLL? should i even write about unnormalized models?
how much detail for sm and ssm? 

As described before, we want to learn the PDF of the data. In the case of PCs we directly model a normalized (integrates to 1)
density but most other approaches model a unnormalized density. This is because the normalization constant is 
very costly or impossible to compute in these scenarios. 

In score matching \cite{score_matching} 

Which leads to this score matching objective: 

% \begin{equation}
%     \label{eq:sm_objective}
%     \mathcal{J}(\theta) = \mathbb{E}_{p(x)} \left[\text{tr} \left( \nabla_x^2 \log p_\theta(x) \right) + \frac{1}{2} \left\| \nabla_x \log p_\theta(x) \right\|^2 \right]
% \end{equation}


However this introduces another problem. With high dimensional data the trace of the hessian is very costly to compute.

Here sliced score matching \cite{sliced_score_matching} is introduced. Instead of doing the costly hessian calculation
the multidimensional data is projected onto n random vectors (slices). The authors in the paper show that ..

Which leads to this final sliced score matching objective: 

% \begin{equation}
%     \label{eq:sm_objective}
%     \mathcal{J}(\theta) = \mathbb{E}_{p(x)} \left[  \left\| \vec{v^T} \nabla_x^2 \log p_\theta(x) \right\|^2 + \frac{1}{2} \left( \vec{v^T} \nabla_x \log p_\theta(x) \right)^@ \right]
% \end{equation}







