\usetikzlibrary{calc}

\renewcommand{\vec}[1]{\textbf{#1}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}


\chapter{Background}
\label{cha:background}

\section{Probabilistic Modelling}
\label{sec:pm}

At it's very core Machine Learning (ML) aims to create algorithms/programs that learn from data in order to reason about it, make future
predictions or perform all kinds of different inference tasks. One possible way to achieve this, especially when there is an inherent 
uncertainty in the data, which is the case for most real-world scenarios, is Probabilistic Modelling. In Probabilistic Modelling we 
use probabilities to express this uncertainty and the rules of probability theory for inference. 

To make this more clear let's say we have some two dimensional data, from variables $X$ and $Y$ and assume that this data was drawn randomly from some unknown distribution. 
We would call this distribution over all possible variables the joint distribution $P^*(X, Y)$. 
If the distribution $P^*(X, Y)$ was known to us all inference tasks would boil down to applying the basic 
rules of probability theory. The sum rule to compute marginals, the product rule to compute conditionals and more complex rules, derived 
from these two, to perform harder inference tasks. 

However since we don't know $P^*(X, Y)$, we have to do something different.
Through machine learning we create a framework that \emph{models} $P(X, Y)$, approximating the unknown true distribution $P^*(X, Y)$, of which we only have limited samples.
We would call a framework, that models the joint distribution a Generative Probabilistic Model. 
After \emph{learning} this model trough the data, which is usually done via Maximum Likelihood Estimation, we can again perform inference using the rules 
of probability theory. And if the model approximates the true distribution close enough we can expect reliable results. \cite{pc_intro}. \\

TODO: explain MLE shortly

Before continuing with different approaches to probabilistic modelling, I want to introduce two key concepts that are often 
used to discuss these different methods. \\

TODO: expressiveness as in expressive efficiency

Definition 1: Expressiveness - a probabilistic model is called expressive if the learned distribution approximates the original distribution to a high degree.
For instance this can be measured with Kullback-Leibler (KL) divergence \cite{kl_divergence}, which measures how similar two probability distributions are. 
Two identical distributions would then have a KL-divergence of $0$. In the Probabilistic Modelling case a KL-divergence of 0 would mean that the model is most exxpressive. \\

Definition 2: Tractability - a probabilistic model is called tractable with respect to an inference task, if the model can complete this 
task exactly and efficiently. Exactly in this case means without relying on approximation e.g. Monte-Carlo Simulation and efficiently 
means in linear time with respect to the model size. \\

In recent years expressive capability has increased considerably through models based on neural networks like Generative-Adversarial
Networks (GANs) \cite{gan}, Variational Autoencoder (VAE) \cite{vae} and many others. However for what these models excel in generating 
very realistic images or compelling text in language modelling, they lack in tractability for all except the most simplest inference tasks. 
\cite{pc_intro} \\

On the other hand there are many mostly older, less complex models like Gaussian Mixture Models (GMMs), Hidden Markos Models (HMMs) and so on, 
that lack in expressiveness and only work well enough for very simple data but can compute most if not all inference tasks tractably.

\section{Probabilistic Circuits}
\label{sec:pc}

Probabilistic Circuit (PC) is an umbrella term for probabilistic models that can perform most inference tasks tractably but can be 
highly expressive at the same time. In general this is achieved by only introducing complexity in a structured manner. More specifically
for a PC to be valid it has to adhere to certain structural properties, but more on that later. \\

Prominent members of the PC class include Cutset Networks \cite{cutset}, Probabilistic
Sentential Decision Diagrams (PSDDs) \cite{psdd}, but I will only be focusing on Sum Product Networks (SPNs) \cite{spn}, which to my 
knowledge has seen the widest adoption. 

In General one can think of a SPN as a structured neural network that consists of leaf nodes, sum nodes and product 
nodes. A SPN then recursively computes weighted mixtures (sum nodes) and factorizations (product nodes) of simple input distributions (leaf nodes). 
These inputs can basically be any probability distribution like Gaussians, Bernoullies, Categroical distributions and so on,
however most of the time we will talk about Gaussians.

Considering this, one could notice that the simplest form of a SPN is just a basic 
Gaussian Mixture Model (GMM), where one sum node mixes two leaf nodes, as depicted in Figure~\ref{fig:spn_gmm}.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[
        node distance=1.5cm and 2.5cm,
        sum/.style={circle, draw=black, thick, minimum size=8mm, inner sep=0pt},
        input/.style={circle, draw=black, thick, minimum size=8mm, inner sep=0pt}
    ]

    % Nodes
    \node[input] (input1) {$X_1$};
    \node[input, right=of input1] (input2) {$X_1$};
    \node[sum, above=1.5cm of $(input1)!0.5!(input2)$] (sum) {$+$};

    % Edges
    \draw[-, thick] (input1) -- node[midway,left] {$w_1$} (sum);
    \draw[-, thick] (input2) -- node[midway,right] {$w_2$} (sum);

    \end{tikzpicture}
    \caption{Simplest SPN}
    \label{fig:spn_gmm}
\end{figure}
Furthermore an only slightly more complex SPN with a product node and two sum node can be seen in Figure~\ref{fig:spn_layered}. 

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[
        node distance=1.5cm and 2.5cm,
        sum/.style={circle, draw=black, thick, minimum size=8mm, inner sep=0pt},
        input/.style={circle, draw=black, thick, minimum size=8mm, inner sep=0pt},
        product/.style={circle, draw=black, thick, minimum size=8mm, inner sep=0pt}
    ]

    % Nodes
    \node[input] (input1) {$X_1$};
    \node[input, right=of input1] (input2) {$X_1$};
    \node[input, right=of input2] (input3) {$X_2$};
    \node[input, right=of input3] (input4) {$X_2$};

    \node[sum, above=1.5cm of $(input1)!0.5!(input2)$] (sum1) {$+$};
    \node[sum, above=1.5cm of $(input3)!0.5!(input4)$] (sum2) {$+$};
    
    \node[product, above=1.5cm of $(sum1)!0.5!(sum2)$] (prod) {$\times$};

    % Edges
    \draw[-, thick] (input1) -- node[midway,left] {$w_1$} (sum1);
    \draw[-, thick] (input2) -- node[midway,right] {$w_2$} (sum1);
    
    \draw[-, thick] (input3) -- node[midway,left] {$w_3$} (sum2);
    \draw[-, thick] (input4) -- node[midway,right] {$w_4$} (sum2);

    \draw[-, thick] (sum1) -- (prod);
    \draw[-, thick] (sum2) -- (prod);

    \end{tikzpicture}
    \caption{Simple SPN}
    \label{fig:spn_layered}
\end{figure}

Altough this is still very very basic it is a great starting point to intruduce and make sense of two central structural 
properties that allows SPNs to be expressive and tractable. 

In the following Definitions PC and SPN can be used interchangebly.

\begin{definition}[Scope]
    If a PC $P$ models the joint distribution of a set of variables $\vec{X}$, each node of $P$ models a distribution over a subset of $\vec{X}$.
    This subset is called the scope of the node. For a leaf node the scope is the input variable to that leaf, for all other nodes the scope 
    is the union of its children's scopes. So the root node always has scope $\vec{X}$. \cite{pc_intro}
\end{definition}

\begin{definition}[Smoothness]
    A sum node is \textit{smooth} if all its inputs have identical scopes. A PC is smooth if all its sum nodes are smooth. \cite{pc_intro}
\end{definition}
\begin{definition}[Decomposability]
    A product node is \textit{decomposable} if all it's input scopes do not share variables. A PC is decomposable if all of its product nodes are decomposable. \cite{pc_intro}
\end{definition}

In simple terms this basically means that sum nodes are only allowed to have inputs over the same variable and product nodes are
only allowed to have inputs over different variables. The two depicted SPNs are smooth and decomposable.

There are more structural properties that a SPN or PC can fullfill, however these two already guarantee tractable computation of 
marginals (MAR) and conditionals (CON)\cite{pc_intro}, which many other models cannot do and which already is quite enough in many scenarios. 
Generally the more structure a PC has, which means more structural properties it must fulfill, the more inference tasks it can compute 
tractably. 

In \cite{pc_intro} there is an in depth explanation, why these properties guarantee tractable inference 
and further discussion on other properties.

Using a SPN in a real-world scenario then would bacisally entail first deciding on a structure (which properties and leaf 
distributions to use and how the nodes should be aranged in the graph) depending on the task at hand and then do a Maximum Likelihood 
Estimation with the weights of the sum nodes and the parameters of the leaf distributions (e.g. means and variances for a Gaussian). 
This omptimization is usually done via Gradient Descent or the Expectation-Maximization (EM) algorithm, 
which is also very popular with Gaussian Mixture Models. 

It is also noteworthy here that a SPN is a normalized model, which means it models a proper density (that integrates to $1$ over the entire real space). 
This makes the omptimization with Maximum Likelihood very straightforward as the model directly outputs the Likelihood when evaluated at one datapoint.
Therefore we can just minimize the negative of this output, in turn maximizing the Likelihood.
Though normally, as with most models, we model the log-likelihood for numerical stability.


\section{Score Matching}
\label{sec:sm}

TODO: change notatioon of p and s

Score Matching \cite{sm} is a concept that is normally used when dealing with unnormalized models like Energy Based Models (EBMs). Here 
Energy typically just refurs to an unnormalized density. It is noteworthy, that most models represent unnormalized densities, 
because usually there is not a straightforward way to ensure that the model outputs a proper density.
Learning these models through Maximum Likelihood Estimation can be difficult because of the computationally infeasible normalization 
constant, usually called $Z_\theta$.


In Equation\ref{eq:pdf} the relation between a parameterized density $p_\theta$ and an Energy $E_\theta$ is expressed. Notice that calculating the normalization constant would 
mean computing the integral over $E_\theta$.

\begin{equation}
    \label{eq:pdf}
    p_\theta(\vec x) = \frac{e^{-E_\theta(\vec x)}}{Z_\theta}
\end{equation}

Score Matching proposes a workaround by working with the log gradient $\nabla_x \log \ p_\theta(\vec x)$ of the density, 
instead of $p_\theta(\vec x)$. \\

Calculating $\nabla_x \log p_\theta(\vec x)$ results in $- \nabla_x E_\theta(\vec x)$ since 
\[\nabla_x \log p_\theta(\vec x) = \nabla_x \log \ \frac{e^{- E_\theta(\vec x)}}{Z_\theta} = \nabla_x \left( - E_\theta(\vec x) - {Z_\theta} \right) = - \nabla_x E_\theta(\vec x)\]
thus removing $Z_\theta$.\\

$\nabla_x \log p_\theta(\vec x)$ is called the score function giving score matching it's name.
Using the score which I will further refer to as $s_\theta$ the author of \cite{sm} proposes to minimize the Fisher Divergence between 
the scores of the data and the scores of the model, defined as 

\begin{equation}
    \label{eq:fisher}
    J(\theta) = \frac{1}{2} \mathbb{E}_{p_d(x)} \left[ \norm{s_\theta(x) - s_d(x)}^2_2 \right]
\end{equation}

as the objective function. Here $p_d$ and $s_d$ refer to the density of the data and the score of the data respectively.

This objective doesn't depend on the intractable normalization constant, however it depends on the score function of the data 
$s_d(x)$ which is unknown. Furthermore in \cite{sm} it is shown that by partial integration this objective function can be expressed as 

\begin{align}
    \label{eq:sm_objective}
    \mathcal{J}(\theta) & = \mathbb{E}_{p_d} \left[\text{tr} \left( \nabla_x s_\theta(x) \right) + \frac{1}{2} \left\| s_\theta(x) \right\|^2 \right] \notag \\ 
    & = \mathbb{E}_{p_d(x)} \left[\text{tr} \left( \nabla_x^2 \log p_\theta(x) \right) + \frac{1}{2} \left\| \nabla_x \log p_\theta(x) \right\|^2 \right]
\end{align}

which is equivalent to \ref{eq:fisher} plus some additive constant. This final score matching objective doesn't depend on the score of the data $s_d$ anymore and can be used for learning. Here tr() refers to the
trace (sum of the diagonals) of the hessian matrix.

\section{Sliced Score Matching}
\label{sec:ssm}

TODO: change notatioon of p and s

While Score Matching get's rid of the normalization constant $Z_\theta$ as seen above, it introduces another term that can become hard
to compute. To calculate the trace of the hessian in Equation \ref{eq:sm_objective} one derivation needs to be computed for each diagonal element of the hessian.
This basically means that the number of derivations needed equals the dimension of the data. 
While this is fine for low dimensional data it quickly becomes unfeasible when
learning higher dimensional data like images.  

In Sliced Score Matching (SSM) \cite{ssm} the basic idea is to project the high dimensional data onto some random direction $v$ to reduce 
the dimensionality and solve a lower dimensional problem. The number of random directions aka. slices can range from $1$ upward, but 
most of the time $1$ slice should suffice. 

Applying this idea results in the following objective replacing Fisher Divergence from Equation \ref{eq:fisher}: 

\begin{equation}
    \label{eq:ssm}
    \mathcal{J}(\theta; p_v) = \frac{1}{2} \mathbb{E}_{p_v} \mathbb{E}_{p_d} \left[ \left( \vec{v}^T s_\theta(x) - \vec{v}^T s_d(x) \right)^2 \right]
\end{equation}

By using partial integration, similar to what was done in Score Matching, we can get rid of the unkown scores of the data $s_d$:

\begin{equation}
    \label{eq:ssm_final}
    \mathcal{J}(\theta; p_\vec{v}) = \mathbb{E}_{p_v} \mathbb{E}_{p_d} \left[ \vec{v}^T \nabla_\vec{x} s_\theta(\vec{x}) \vec{v} + \frac{1}{2} \left( \vec{v}^T s_\theta(\vec{x}) \right)^2 \right]
\end{equation}

which is again equivalent to \ref{eq:ssm} plus some additive constant \cite{ssm}. This objective can be used for learning, however
it is worth to point out that when $p_v$ is a multivariate standard normal or multivariate Rademacher distribution, 
$\mathbb{E}_{p_v} [ ( \vec{v}^T s_\theta(\vec{x}) )^2 ] = \norm{s_\theta(\vec{x})}_2^2$ in which case the second term 
of \ref{eq:ssm_final} can be integrated analytically \cite{ssm}, yielding the following objective.

\begin{equation}
    \label{eq:ssm_vr}
    \mathcal{J}(\theta; p_\vec{v}) = \mathbb{E}_{p_v} \mathbb{E}_{p_d} \left[ \vec{v}^T \nabla_\vec{x} s_\theta(\vec{x}) \vec{v} + \frac{1}{2} \norm{ s_\theta(\vec{x}) }^2_2 \right]
\end{equation}

The authors of \cite{ssm} refer to this objective as 
Sliced Score Matching with Reduced Variance (SSM-VR), which according to them produces better performance than the standard SSM objective as 
in \ref{eq:ssm_final}.





