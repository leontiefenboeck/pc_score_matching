\renewcommand{\vec}[1]{\textbf{#1}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}


\chapter{Background}
\label{cha:background}

\section{Probabilistic Modelling}
\label{sec:pm}

In Machine Learning (ML) one of the core disciplines is modelling data in some manner to be able to 
extract information, make predictions and perform all sorts of tasks of inference. 
One way to achieve this is to use probability theory as a framework which provides a sound and consistent way to
reason under uncertainty. We assume that the data was randomly  drawn from some underlying unknown distribution
\emph{out there} and we try to learn the distribution back from the data. \cite{pc_intro}

I will get to how this can be learned in the future sections.




\section{Probabilistic Circuits}
\label{sec:pc}

Recent progress in probabilistic modelling through .. have pushed the expressive capability (how close the 
approximated distribution is to the true one) ever forward. However what expressive gains these models achieve through 
their complexity they loose in tractability, the ability to perform inference tasks exactly and efficiently.
More concretely tractability in one inference task means that a model can perform the task exactly, without approximation, and in polynomial time with respct to the model size.
Except for the simplest taks, these models must rely on costly approximation.

On the other hand older approaches like .. which are less complex and therefore lack in expressiveness can perform 
all sort of inference tasks tractably.

Probabilistic Circuits are a framework for probabilistic models that try to be both expressive and tractable. 
In essence they try to achieve this by only introducing complexity in a controlled manner. To be valid a PC must 
adhere to certain structural constraints. 

\section{Sliced Score Matching}
\label{sec:ssm}

Why score matching instead of minimizing NLL? should i even write about unnormalized models?
how much detail for sm and ssm? 

As described before, we want to learn the PDF of the data. In the case of PCs we directly model a normalized (integrates to 1)
density but most other approaches model a unnormalized density. This is because the normalization constant is 
very costly or impossible to compute in these scenarios. 

In score matching \cite{score_matching} 

Which leads to this score matching objective: 

\begin{equation}
    \label{eq:sm_objective}
    \mathcal{J}(\theta) = \mathbb{E}_{p(x)} \left[\text{tr} \left( \nabla_x^2 \log p_\theta(x) \right) + \frac{1}{2} \left\| \nabla_x \log p_\theta(x) \right\|^2 \right]
\end{equation}


However this introduces another problem. With high dimensional data the trace of the hessian is very costly to compute.

Here sliced score matching \cite{sliced_score_matching} is introduced. Instead of doing the costly hessian calculation
the multidimensional data is projected onto n random vectors (slices). The authors in the paper show that ..

Which leads to this final sliced score matching objective: 

\begin{equation}
    \label{eq:sm_objective}
    \mathcal{J}(\theta) = \mathbb{E}_{p(x)} \left[  \left\| \vec{v^T} \nabla_x^2 \log p_\theta(x) \right\|^2 + \frac{1}{2} \left( \vec{v^T} \nabla_x \log p_\theta(x) \right)^@ \right]
\end{equation}







