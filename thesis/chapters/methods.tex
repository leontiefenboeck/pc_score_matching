\renewcommand{\vec}[1]{\textbf{#1}}
\renewcommand{\norm}[1]{\left\lVert#1\right\rVert}

\chapter{Methods}
\label{cha:methods}

As previously mentioned in the introduction, the main goal of this thesis is to train
Probabilistic Circuits (PCs) through novel ways, specifically Score Matching (SM) and Sliced Score Matching (SSM).
To then compare results from these methods with results from conventional methods to train PCs, specifically Maximum Likelihood Estimation (MLE) using both Gradient Descent (GD) and Expectation Maximization (EM). \\

\section{Creating a simple PC}
\label{sec:simple_pc}

Recall from Section \ref{sec:pc} that the simplest variant of a PC computes the weighted sum of two input distributions. This is 
called a Mixture Model and the graph can be seen in Figure \ref{fig:spn_gmm}. If we expand this to a mixture 
of $K$ components and use Gaussian distributions in the leaf nodes then we arrive at the Gaussian Mixture Model 
discussed in Section \ref{sec:gmm} with the modelled density function from Equation \ref{eq:gmm}.

However when implementing this, we want to model the log-density $\log p_{\boldsymbol{\theta}}(\vec x)$ since it is easier to work with. So 
we take the $\log$ of Equation \ref{eq:gmm}:

\begin{equation}
    \log p_{\boldsymbol{\theta}}(\vec x) = \log \sum_{k=1}^K \pi_k \mathcal{N}(\vec x|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) 
    \label{eq:gmm_log}
\end{equation}

We also want to calculate a single weighted Gaussian component in log-space, so inside the sum, we take 
the exponential of the logarithm, which is valid since it would cancel out. 

\begin{align}
    \log p_{\boldsymbol{\theta}}(\vec x) & = \log \sum_{k=1}^K \exp \left( \log \left(\pi_k \mathcal{N}(\vec x|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \right) \right) \notag \\
    & = \log \sum_{k=1}^K \exp \left( \log (\pi_k) + \log \left( \mathcal{N}(\vec x|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \right) \right)
    \label{eq:gmm_log2}
\end{align}

To compute this exactly as in Equation \ref{eq:gmm_log2}, we encounter a challenge where the calculation of exponentials, can lead to instability due to overflow or underflow \cite{logsumexp}. \\
To address this, we use the \textbf{log-sum-exp (LSE)} \cite{logsumexp} trick:

\[
    \log \sum_{i=k}^K \exp(x_k) = C + \log \sum_{i=k}^K \exp(x_k - C)
\]

Introducing this constant $C$, where usually $C = \max_k (x_k)$, mitigates underflow \cite{logsumexp}. Overflow is still possible 
but typically that is less of an issue. \\

Let $x_k = \log(\pi_k) + \log \left(\mathcal{N}(\vec x | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)\right)$ and 
let $C = \max_k \left( \log(\pi_k) + \log \left( \mathcal{N}(\vec x | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \right) \right) $ we can rewrite 
Equation \ref{eq:gmm_log2} to arrive at a final numerically stable expression: 

\begin{equation}
    \log p_{\boldsymbol{\theta}}(\vec x) = C + \log \sum_{k=1}^K \exp\left( \log(\pi_k) + \log \left( \mathcal{N}(\vec x | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \right) - C \right)
    \label{eq:gmm_log_final}
\end{equation}

We implemented this expression in python and used PyTorch's \cite{pytorch} \texttt{torch.nn.Parameter}
for all learnable parameters $\boldsymbol{\theta}$ to use PyTorch's optimization framework later.

We used \texttt{torch.distributions.MultivariateNormal} to compute the log density of a single 
gaussian component, though we could have also used this expression:
\begin{equation}
    \log p(\mathbf{x}) = -\frac{n}{2} \log(2\pi) - \frac{1}{2} \log |\boldsymbol{\Sigma}| - \frac{1}{2} (\mathbf{x} - \boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu})
\end{equation}

We used \texttt{torch.logsumexp} for the LSE operation.  

Note that for readability we will further refer to the whole density function of the GMM from Equation \ref{eq:gmm_log_final} as just 
$\log p_{\boldsymbol{\theta}}(\vec x)$.

\subsection{Maximum Likelihood Estimation}
\label{sec:gmm_mle}

Now to train this GMM on some data $\mathcal{X} = \{\vec x_1, ..., \vec x_N\}$ in the conventional way, we can calculate $\boldsymbol{\theta}$ using the 
log Maximum Likelihood Estimation (MLE) objective from Equation \ref{eq:log_mle} in the Background chapter with the density function
of the GMM from Equation \ref{eq:gmm_log_final}:
\begin{align}
    \max_{\substack{\boldsymbol{\theta}}} L(\boldsymbol{\theta}) := \sum_{i=1}^{N} \log p_{\boldsymbol{\theta}}(\vec x_i)
    \label{eq:gmm_objective}
\end{align}

In plain text this means maximizing $\sum_{i=1}^{N} \log p_{\boldsymbol{\theta}}(\vec x)$ with respect to $\boldsymbol{\theta}$, which consists of the weights $\boldsymbol \pi$, the means $\boldsymbol \mu$ and the covariance matrices $\boldsymbol \Sigma$.

Now with the objective formulated we will introduce Gradient Descent (GD) and Expectation Maximization (EM), which are learning algorithms
that provide a concrete way on how the best possible $\boldsymbol{\theta}$ can be found. 

\subsubsection{Gradient Descent}
\label{sec:gmm_sgd}

Gradient Descent (GD) is an iterative optimization algorithm used to minimize a given objective function, by  
computing the gradients of the function with respect to its parameters and updating them iteratively \cite{deepl}.

Recall that however when training a GMM, the goal is to \textbf{maximize} the log-likelihood function defined in Equation \ref{eq:gmm_objective}. 
So to do this using GD, we instead \textbf{minimize} the \textbf{negative} log-likelihood (NLL), which is equivalent.

The main steps in applying GD to our GMM objective are: 

1. \textbf{Computing the gradients} of the negative Log-Likelihood function $\text{NLL}(\boldsymbol{\theta})$ for the parameters $\boldsymbol{\theta}$: 
\[
    \nabla_{\boldsymbol{\theta}} \text{NLL}(\boldsymbol{\theta}) = \nabla_{\boldsymbol{\theta}} (- \sum_{i=1}^{N} \log p_{\boldsymbol{\theta}}(\vec x))
\]

2. \textbf{Updating the parameters} using the computed gradients multiplied by a learning rate $\eta$:
\[
    \boldsymbol{\theta}^{(t+1)} = \boldsymbol{\theta}^{(t)} - \eta \nabla_{\boldsymbol{\theta}} \text{NLL}(\boldsymbol{\theta})
\]

We repeat this for $T$ training iterations, also often called epochs. \\
Note that for the first iteration the parameters $\boldsymbol{\theta}$ have to be initialized in some manner, e.g. randomly. 

\begin{algorithm}
    \caption{Gradient Descent}
    \label{alg:gd_gmm}
    \begin{algorithmic}[1]  
        \Require $\vec X = \{\vec x_1, \vec x_2, ..., \vec x_N\}$, $\boldsymbol{\theta}$, $\eta$, $T$
        \For{$t = 1$ to $T$}
            \State $\text{NLL}(\boldsymbol{\theta}) \gets - \sum_{i=1}^{N} \log p_{\boldsymbol{\theta}}(\vec x_i)$
            \State $\boldsymbol{\theta} \gets \boldsymbol{\theta} - \eta \nabla_{\boldsymbol{\theta}} \text{NLL}(\boldsymbol{\theta})$
        \EndFor
        \State \Return $\boldsymbol{\theta}$
    \end{algorithmic}
\end{algorithm}

We implemented this using PyTorch's \cite{pytorch} \texttt{autograd} capabilities to compute the gradients.

\newpage
\subsubsection{Expectation Maximization}
\label{sec:gmm_em}

Expectation Maximization (EM) is another method used for iterative optimization, especially popular with Gaussian Mixture Models. EM achieves this by alternating between the \textbf{Expectation (E) step} and the \textbf{Maximization (M) step} \cite{ml_book}.

1. \textbf{Expectation Step (E-Step):} In the E-step, we compute the posterior probability (also called responsibilities) that a given data point $\vec{x}_i$ belongs to each Gaussian component $k$:
\[
   \gamma_{ik} = \frac{\pi_k \mathcal{N}(\vec{x}_i|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}{\sum_{j=1}^{K} \pi_j \mathcal{N}(\vec{x}_i|\boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)}
\]

2. \textbf{Maximization Step (M-Step):} In the M-step, we update the parameters $\pi_k$, $\boldsymbol{\mu}_k$, and $\boldsymbol{\Sigma}_k$ based on the posterior probabilities from the E-step: 
\[
   \pi_k = \frac{1}{N} \sum_{i=1}^{N} \gamma_{ik}, \quad
   \boldsymbol{\mu}_k = \frac{\sum_{i=1}^{N} \gamma_{ik} \vec{x}_i}{\sum_{i=1}^{N} \gamma_{ik}}, \quad
   \boldsymbol{\Sigma}_k = \frac{\sum_{i=1}^{N} \gamma_{ik} (\vec{x}_i - \boldsymbol{\mu}_k)(\vec{x}_i - \boldsymbol{\mu}_k)^T}{\sum_{i=1}^{N} \gamma_{ik}}
\]

We again repeat this for $T$ training iterations, and initialize $\boldsymbol{\theta}$ for the first one.  \\

\begin{algorithm}
    \caption{Expectation Maximization}
    \label{alg:em_gmm}
    \begin{algorithmic}[1]  
        \Require $\vec X = \{\vec x_1, \vec x_2, ..., \vec x_N\}$, $\boldsymbol{\theta}$, $T$
        \For{$t = 1$ to $T$}
            \For{$i = 1$ to $N$}
                \For{$k = 1$ to $K$}
                    \State $\gamma_{ik} \gets \frac{\pi_k \mathcal{N}(\vec{x}_i|\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}{\sum_{j=1}^{K} \pi_j \mathcal{N}(\vec{x}_i|\boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)}$
                \EndFor
            \EndFor
            \For{$k = 1$ to $K$}
                \State $\pi_k \gets \frac{1}{N} \sum_{i=1}^{N} \gamma_{ik}$
                \State $\boldsymbol{\mu}_k \gets \frac{\sum_{i=1}^{N} \gamma_{ik} \vec{x}_i}{\sum_{i=1}^{N} \gamma_{ik}}$
                \State $\boldsymbol{\Sigma}_k \gets \frac{\sum_{i=1}^{N} \gamma_{ik} (\vec{x}_i - \boldsymbol{\mu}_k)(\vec{x}_i - \boldsymbol{\mu}_k)^T}{\sum_{i=1}^{N} \gamma_{ik}}$
            \EndFor
        \EndFor
        \State \Return $\boldsymbol{\theta}$
    \end{algorithmic}
\end{algorithm}

\newpage
\subsection{Score Matching}
\label{sec:gmm_sm}

Training the GMM with Score Matching is actually not very different from the MLE with Gradient Descent from above, we just need 
to switch the objective function.  

\subsubsection{Exact Score Matching}

We take the Exact Score Matching objective function from Equation \ref{eq:sm_objective}, Section \ref{sec:sm} and formulate the optimization problem:

\begin{equation}
    \min_{\substack{\boldsymbol{\theta}}} \hat{\mathcal{L}}(\boldsymbol{\theta}) = \frac{1}{N} \sum_{i=1}^{N} \left(\text{tr} \left( \nabla_{\vec x_i} s_{\boldsymbol{\theta}}(\vec x_i) \right) + \frac{1}{2} \left\| s_{\boldsymbol{\theta}}(\vec x_i) \right\|^2 \right)
    \label{eq:sm_optimization_problem}
\end{equation}

Then we optimize this objective using Gradient Descent for $T$ training iteration and learning rate $\eta$.  

\begin{algorithm}
    \caption{Score Matching}
    \begin{algorithmic}[1]  
        \Require $\vec X = \{\vec x_1, \vec x_2, ..., \vec x_N\}$, $\boldsymbol{\theta}$, $\eta$, $T$
        \For{$t = 1$ to $T$}
            \State $\mathcal{L}(\boldsymbol{\theta}) \gets \frac{1}{N} \sum_{i=1}^N \textsc{ScoreMatchingLoss}(\vec x_i)$
            \State $\boldsymbol{\theta} \gets \boldsymbol{\theta} - \eta \nabla_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta})$
        \EndFor
        \State \Return $\boldsymbol{\theta}$
        \end{algorithmic}
        \noindent\rule{\linewidth}{0.4pt}
        \textbf{Procedure:} \textsc{ScoreMatchingLoss}($\vec x$)
        \begin{algorithmic}[1]
            \State $s_{\boldsymbol{\theta}}(\vec x) \gets \nabla_{\vec x} \log p_{\boldsymbol{\theta}}(\vec x)$
            \State $\mathcal{L}(\boldsymbol{\theta}) \gets tr(\nabla_{\vec x} s_{\boldsymbol{\theta}}(\vec x)) + \frac{1}{2} \left\| s_{\boldsymbol{\theta}}(\vec x) \right\|^2 $
            \State \Return $\mathcal{L}(\boldsymbol{\theta})$
        \end{algorithmic}
\end{algorithm}

\subsubsection{Sliced Score Matching}
\label{sec:gmm_ssm}

To use the sliced version of Score Matching \ref{sec:ssm} we can do everything in the same way as above, we only need to replace 
the objective function with the Sliced Score Matching objective from Equation \ref{eq:ssm_vr}, Section \ref{sec:ssm}. \\
Note that for simplicity we only use one random vector $\vec v_i$ (one slice) for each data point. 

\begin{equation}
    \min_{\substack{\boldsymbol{\theta}}} \mathcal{L}(\boldsymbol{\theta}) = \frac{1}{N} \sum_{i=1}^{N} \left( \vec{v}_i^T \nabla_{\vec x_i} s_{\boldsymbol{\theta}}(\vec x_i) \vec{v}_i + \frac{1}{2} \norm{  s_{\boldsymbol{\theta}}(\vec x_i) }^2_2 \right)
    \label{eq:ssm_optimization_problem}
\end{equation}

\begin{algorithm}[H]
    \caption{Sliced Score Matching}
    \begin{algorithmic}[1]  
        \Require $\vec X = \{\vec x_1, \vec x_2, ..., \vec x_N\}$, $\boldsymbol{\theta}$, $\eta$, $T$
        \For{$t = 1$ to $T$}
            \State $\mathcal{L}(\boldsymbol{\theta}) \gets \frac{1}{N} \sum_{i=1}^N \textsc{SlicedScoreMatchingLoss}(\vec x_i)$
            \State $\boldsymbol{\theta} \gets \boldsymbol{\theta} - \eta \nabla_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta})$
        \EndFor
        \State \Return $\boldsymbol{\theta}$
        \end{algorithmic}
        \noindent\rule{\linewidth}{0.4pt} 
        \textbf{Procedure:} \textsc{SlicedScoreMatchingLoss}($\vec x$)
        \begin{algorithmic}[1]
            \State $\vec v \gets \mathcal{N}(0, I)$
            \State $s_{\boldsymbol{\theta}}(\vec x) \gets \nabla_{\vec x} \log p_{\boldsymbol{\theta}}(\vec x)$
            \State $\mathcal{L}(\boldsymbol{\theta}) \gets \vec{v}^T \nabla_{\vec x_i} s_{\boldsymbol{\theta}}(\vec x_i) \vec{v} + \frac{1}{2} \norm{  s_{\boldsymbol{\theta}}(\vec x_i) }^2_2$
            \State \Return $\mathcal{L}(\boldsymbol{\theta})$
        \end{algorithmic}
\end{algorithm}

We implemented both Algorithms again using PyTorch \cite{pytorch} and \texttt{torch.autograd}. 

\subsection{Sampling}
\label{sec:gmm_sampling}

Once the Gaussian Mixture Model (GMM) is trained, we can generate new samples from the learned distribution. Sampling from a GMM involves two steps:

1. \textbf{Component Selection}: First, we sample a component index $k$ from a categorical distribution with the mixture weights $\boldsymbol{\pi}$ as probabilities.
   
2. \textbf{Gaussian Sampling}: After selecting a component $k$, we sample from the corresponding Gaussian with parameters $\boldsymbol{\mu}_k$ and $\boldsymbol{\Sigma}_k$. 

\begin{algorithm}
    \caption{GMM Sampling}
    \label{alg:gmm_sampling}
    \begin{algorithmic}[1]  
        \Require $\boldsymbol{\theta} = \{\boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Sigma}\}$
            \State $ k \sim \textsc{Categorical}(\boldsymbol{\pi})$
            \State $ x \sim \mathcal{N}(\boldsymbol{\mu_k}, \boldsymbol{\Sigma_k})$
        \State \Return $x$
    \end{algorithmic}
\end{algorithm}

We implemented this in PyTorch \cite{pytorch}, using \\
\texttt{torch.distributions.MultivariateNormal} for generating samples,\\
and \texttt{torch.Categorical} for component selection.

\section{Using more complex PCs}

While the model proposed in Section \ref{sec:simple_pc} works well for relatively simple tasks like two dimensional density estimation,
it doesn't perform very well with complex higher dimensional data, e.g. images. 

So to get a wider variety of results we decided to also use a state-of-the-art framework for Probabilistic Circuits called 
EinsumNetworks \cite{einsum} for modelling distributions over more complex higher dimensional datasets. 

For more details on EinsumNetworks we refer to \cite{einsum}. In general, the same 
principles as in our GMM apply but due to the deeper structure, the model is much more complex and can approximate more complex distributions. 
It also has a lot more trainable parameters and thus training takes more time. 

However implementing the training of an EinsumNetwork with our targeted algorithms is basically the same 
as in the GMM case. Note that training with EM already comes with the framework, so we will skip going into detail about this. \\
For all other algorithms, we can apply them in the exact same way as described in Section \ref{sec:simple_pc} above, only needing 
to switch out the calculation of $\log p_{\boldsymbol{\theta}}(\vec x)$ to the log-density function the EinsumNetwork framework provides. 
