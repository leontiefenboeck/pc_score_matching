\newcommand{\src}{\mathcal{D}_s}
\newcommand{\tar}{\mathcal{D}_t}
\newcommand{\x}{\mathbf{x}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\mub}{\mathbf{\mu}}
\newcommand{\Sigmab}{\mathbf{\Sigmab}}
\newcommand{\lsimple}{\mathcal{L}_{\text{simple}}}
\newcommand{\be}{\bm{\epsilon}}


\chapter{Methods}
\label{cha:methods}

In this work, we attack an unprotected implementation of the \textit{Advanced Encryption Standard} (AES), a popular symmetric-key encryption algorithm \cite{aes}.
We are given a dataset $\dat = \{(\leak^{(i)}, \kk^{(i)}, \pp^{(i)})\}_{i=1}^n$ which contains $n$ samples which each consist of a leakage $\leak^{(i)} \in \mathbb{R}^d$, a $b$-byte key $\kk^{(i)}$, and a $b$-byte plaintext $\pp^{(i)}$. 
Often, we will reason about individual bytes in $\kk$ and $\pp$, denoted $k_j, p_j \in \mathbb{F}_2^8$, $1 \leq j \leq b$. 
Further, assume we have access to an algorithm, that, given some $\leak$, produces probability mass functions over intermediate variables, i.e., $p(v_j \mid \leak)$, where $v_j$ is an arbitrary intermediate variable.

\section{Problem Description}
Recall from Section \ref{sec:aes_explanation} that in the AES, the key schedule produces keys $\kk^0,\dots,\kk^{10}$ where $\kk^0$ is just the key $\kk$. Thus, before executing the first round of AES, we call \textsc{AddRoundKey}, which computes the \textsc{xor} between the key $\kk$ and the plaintext $\pp$. Thus, this operation, followed by the first round of AES, is a popular attack target as it directly involves the key we wish to recover, and the plaintext we assume to know. We also emphasize that we do not attack the key schedule in our work.
Algorithm \ref{alg:aes} shows the first operations of AES encryption (up to the first \textsc{MixColumns} call), which will be the target of this work. Moreover, we skip the first \textsc{ShiftRows} operation, since it merely corresponds to a fixed re-labeling of the input bytes (e.g., the true bytes $k_6, p_6$ are called $k_2, p_2$) and does not affect probabilistic inference.

\begin{algorithm}[ht]
    \caption{Simplified Beginning of AES-128}\label{alg:aes}
    \begin{algorithmic}[1]
    \Require Key bytes $k_1,\dots,k_{16}$, Plaintext bytes $p_1,\dots,p_{16}$
    \For{$i$ in $1,\dots,16$}
        \State $y_i \gets k_i \oplus p_i$ \Comment{\textsc{AddRoundKey}}
        \State $x_i \gets S(y_i)$ \Comment{\textsc{SubBytes}}
    \EndFor

    \Comment{We skip \textsc{ShiftRows} because it only corresponds to re-labeling $k_i, p_i$}

    \For{$i$ in $0,4,8,12$}
        \State $x_{i+1}^{(m)},\dots,x_{i+4}^{(m)} \gets \textsc{MixColumn}(x_{i+1},\dots,x_{i+4})$ \Comment{\textsc{MixColumns}}
    \EndFor
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[ht]
    \caption{\textsc{MixColumn}}\label{alg:mc}
    \begin{algorithmic}[1]
    \Require Input bytes $x_1,\dots,x_{4}$
    \State $x_{12}, \ x_{23}, \ x_{34}, \ x_{41} \gets (x_1 \oplus x_2), (x_2 \oplus x_3), (x_3 \oplus x_4), (x_4 \oplus x_1)$
    \State $g \gets x_{12} \oplus x_{34}$

    \State $\tilde{x}_{12}, \ \tilde{x}_{23}, \ \tilde{x}_{34}, \ \tilde{x}_{41} \gets \textsc{xtime}(x_{12}), \textsc{xtime}(x_{23}), \textsc{xtime}(x_{34}), \textsc{xtime}(x_{41})$

    \State $x'_{12}, \ x'_{23}, \ x'_{34}, \ x'_{41} \gets (\tilde{x}_{12} \oplus g), (\tilde{x}_{23} \oplus g), (\tilde{x}_{34} \oplus g), (\tilde{x}_{41} \oplus g)$

    \State $x^{(m)}_{1}, \ x^{(m)}_{2}, \ x^{(m)}_{3}, \ x^{(m)}_{4} \gets (x_1 \oplus x'_{12}), (x_2 \oplus x'_{23}), (x_3 \oplus x'_{34}), (x_4 \oplus x'_{41})$
    \State \Return $x^{(m)}_{1}, \ x^{(m)}_{2}, \ x^{(m)}_{3}, \ x^{(m)}_{4}$
    \end{algorithmic}
\end{algorithm}

As discussed in Section \ref{sec:sasca}, when performing a SASCA, we need to convert the algorithmic description into a \textit{factor graph}. When doing so, the resulting factor graph (shown in Figure \ref{fig:aes_fg}) turns out to be \textit{cyclic}/\textit{loopy}.
The reason for this is the structure of the \textsc{MixColumns} routine. As described in Section \ref{sec:bp}, directly running belief propagation (BP) to compute marginals in this graph results in an approximate solution. 

\begin{figure}[ht]
    \centering
    \input{figures/aes_factor_graph.tex}
    \caption{Simplified factor graph showing operations on the first 4 bytes key and plaintext bytes in AES (Algorithm \ref{alg:aes}). Importantly, note that this graph contains \textit{cycles}. Every variable node $v$ (except $p_i$, which is assumed to be known) has an additional factor $p(v | \leak)$, which we hide since it impairs clear visualization.}
    \label{fig:aes_fg}
\end{figure}


A key insight of our approach is the fact that we can, in some cases, \textit{compile the loopy parts of the graph into a structure that supports tractable marginalization}. This effectively \textit{lumps} together loopy parts of the factor graph into a \textit{single} factor, resulting in an \textit{acyclic} factor graph (Figure \ref{fig:aes_acyclic_fg}).

\begin{figure}[h!]
    \centering
    \input{figures/aes_factor_graph_acyclic.tex}
    \caption{We aggregate the loopy \textsc{MixColumn} subgraph into a single (large) boolean factor $\mathcal{M}_B$. In this view, we show the additional posterior distribution factors hidden in Figure \ref{fig:aes_fg}. To simplify visualization, we use \textit{plate} notation: Each subgraph in a rectangle repeats as often as specified.}
    \label{fig:aes_acyclic_fg}
\end{figure}

Note that simply abstracting a loopy subgraph as a factor node is of no use if we do not have efficient means to perform inference \textit{within} the introduced node: Assume that given the factor graph in Figure \ref{fig:aes_acyclic_fg}, we wish to marginalize out all variables except $k_i$. Let $\vv$ be the collection of variables involved in the \textsc{MixColumns} computation, i.e.,
\begin{align}
    \vv = ( \underbrace{x_1,\dots,x_4}_{\text{Inputs } \vin}, \ \underbrace{x_{12},\dots,x'_{41}}_{\text{Intermediates } \vmid}, \ \underbrace{x_1^{(m)},\dots,x_4^{(m)}}_{\text{Outputs } \vout} )
\end{align}
To use the notation defined in Section \ref{sec:asca}, $\mathcal{M}_B(\vv)$ is the boolean representation of \textsc{MixColumns}, evaluating to $1$ if $\vv$ is consistent with \textsc{MixColumn}, and to $0$ else.

As illustrated in Equation \ref{eq:bp_ftv}, the message that the $\mathcal{M}_B$ node must send to its neighbour $x_i \in \vin$ is given by
\begin{align}
\label{eq:message}
    \mu_{\mathcal{M}_B \to x_i}(x_i) &= \sum_{\vv \setminus x_i} \mathcal{M}_B(\vv) \cdot \prod_{v_j \in \vv \setminus x_i} p(v_j | \leak)
\end{align}
For a single value of $x_i$, computing this loop in a na\"ive manner (i.e., iterating over all combinations of $\vv$) would take $2^{136}$ loop iterations. However, we do not need to take into account combinations of $\vv$ that are inconsistent with \textsc{MixColumn}. Instead, we could, for a fixed value of $x_i$, loop over all possible combinations of \textit{remaining input bytes} $\vin \setminus x_i$, compute their corresponding intermediate and output values, and only consider these assignments of $\vv$ (since all other assignments yield $M_B(\vv) = 0$ and thus, do not contribute to the sum). Formally, with 
\begin{align}
    \mathcal{V}_i(y) &= \{ \vv \mid \mathcal{M}_B(\vv) = 1 \land x_i = y \}
\end{align}
compute
\begin{align}
    \label{eq:loop}
    \mu_{\mathcal{M}_B \to x_i}(x_i) &= \sum_{\vv \in \mathcal{V}_i(x_i)} \prod_{v_j \in \vv \setminus x_i} p(v_j | \leak)
\end{align}
For all values $x_i \in \{0,\dots,255\}$, this takes $2^{32}$ loop iterations in total, where each iteration takes time $O(N + \operatorname{time}(\textsc{MixColumn}))$, where $N$ is the number of bytes in $\vv$. Since we are only interested in marginals w.r.t. key bytes, we only need to compute messages for $x_1,\dots,x_4$, thus running this procedure $4$ times. 

Again, this is a na\"ive approach as (1) the computational complexity grows exponentially with the number of inputs, and, importantly, (2) we do not take advantage of the \textit{structure} of $\mathcal{M}_B$, treating it as a black box.\footnote{Here, we only use the fact that $\mathcal{M}$ is a deterministic function with inputs $x_1,\dots,x_4$.}

\section{Message Computation as Circuit Multiplication}
Consider the message computation in Equation \ref{eq:message}. To compute the message for some $x_i$, we wish to represent both $\mathcal{M}_B(\vv)$ and all probability mass functions (PMFs) $p(v_j | \leak), v_j \in \vv \setminus x_i$ as probabilistic sentential decision diagrams (PSDDs). Afterwards, we could compute the \textit{circuit product} of these PSDDs and marginalize out all variables except $x_i$ in linear time in the size of the resulting circuit.

We leverage existing SDD compilation techniques \cite{dynamic_min_choi} to find a vtree $v_{\mathcal{M}_B}$ for a CNF representation of $\mathcal{M}_B$ such that the corresponding SDD is succinct. We then project $v_{\mathcal{M}_B}$ onto smaller vtrees whose scope is always a single byte. Next, we compile each PMF $p(v_j | \leak)$ into a PSDD that respects the corresponding projected vtree for this byte. This ensures that all circuits are \textit{compatible}. Figure \ref{fig:compilation} sketches the end-to-end compilation process.

This approach differs from existing compilation techniques for probabilistic graphical models (PGMs) \cite{sbn, tractable_ops} in the way that we first perform a vtree search such that the SDD compilation of $\mathcal{M}_B$ is tractable in the first place.

\begin{figure}[ht]
    % \centering
    \makebox[\textwidth][c]{
        \scalebox{0.70}{
            \input{figures/compilation.tex}
        }
    }%

    \caption{Given the algorithmic description of \textsc{MixColumn} ($\mathcal{M}$) and the posterior distributions $p(\cdot | \leak)$ as inputs to our compilation pipeline, we end up with PSDD representations of all input distributions, denoted $\pc(\cdot)$. Moreover, all PSDDs are pairwise \textit{compatible} for downstream circuit multiplication tasks.}
    \label{fig:compilation}
\end{figure}

\section{CNF Representation of \textsc{MixColumn}}
% pyeda, multidimensional arrays of boolean variables & optimization
% size of resulting CNF (number of variables, number of clauses)
Given the algorithmic description of $\mathcal{M}$ (Algorithm \ref{alg:mc}), we wish to construct a boolean representation $\mathcal{M}_B(\vv)$ in CNF.
For this task, we use the Python package \texttt{pyeda} \cite{pyeda} which supports logic minimization and symbolic boolean algebra on multivariate arrays of boolean variables. In essence, we replace all variable assignments in Algorithm \ref{alg:mc} with equality constraints to obtain a boolean representation of $\mathcal{M}$. Since we model all $21$ bytes in $\vv$ (and do not introduce further auxiliary variables), the resulting CNF expression reasons over $21 \cdot 8 = 168$ boolean variables. We find that the final CNF expression consists of $648$ clauses and has a \textit{size} of $2649$, where \texttt{pyeda} defines \textit{size} recursively: Each literal has size $1$ and each operator ($\land, \lor$) has size equal to the sum of the sizes of its children plus $1$. The average number of literals within a clause is $3.09$.

\section{SDD Compilation}
% cite sdd compiler from darwiche
% dynamic vtree search
To compile our CNF $\mathcal{M}_B$ into an SDD, we leverage the open-source SDD compiler suite\footnote{\url{http://reasoning.cs.ucla.edu/sdd/}} by Choi and Darwiche. More precisely, we utilize both the \textit{bottom-up} compiler introduced in \cite{dynamic_min_choi}, as well as the \textit{top-down} compiler in \cite{top_down_comp}. The bottom-up compiler implements a dynamic vtree search algorithm, which heuristically searches through the space of vtrees to minimize the size of the resulting SDD \cite{dynamic_min_choi}. The compiler takes an initial vtree as input, which will be modified during compilation. We compile $\mathcal{M}_B$ using a variant of different initial vtrees and find that a vtree constructed with the top-down compiler \cite{top_down_comp} (\texttt{miniC2D}\footnote{\url{http://reasoning.cs.ucla.edu/minic2d/}}, using a hypergraph with fixed balance factor) empirically produces the most succinct SDD representation with a size of $\approx 12000$ edges ($\approx 4000$ nodes) and takes $< 10$ seconds to compile on a modern laptop CPU.

% compilation time        : 4.418 sec
%  sdd size               : 12,140 
%  sdd node count         : 4,121 
%  sdd model count        : 4,294,967,296    0.001 sec

\section{Compiling PMFs into PSDDs}

Let $\zz \in \mathbb{F}^8_2$ be a bitstring of length $8$ (byte) with individual bits $z_1,\dots,z_8$.
Given a probability mass function (PMF) $p(\zz)$ and a vtree $v$ over $z_1,\dots,z_8$, we wish to produce a PSDD that respects $v$ and represents $p(\zz)$ \textit{exactly}. In essence, we can achieve this by recursively computing conditionals of $p$ according to the structure of $v$. A simple example is shown in Figure \ref{fig:pmf_comp} and a simplified pseudocode algorithm for compiling an arbitrary PMF into a PSDD is given in Appendix \ref{app:pmf_to_psdd_compilation}.

\begin{figure*}[ht!]
    \begin{subfigure}[t]{0.2\textwidth}
        \vspace{-4.0cm}
        \centering
        \input{figures/vtree_comp.tex}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.8\textwidth}
        % \vspace{-3.2cm}
        % \centering
        \input{figures/pc_pmf_compilation.tex}
    \end{subfigure}
    \caption{A vtree over 3 bits $A,B,C$ (left) and a PSDD respecting this vtree (right) representing an arbitrary PMF $p(A,B,C)$. We use the factorization $p(A,B,C) = p(C | A,B) \cdot p(A,B)$ and set $\theta_1 = p(C | \neg A, \neg B), \theta_2 = p(C | \neg A, B), \theta_3 = p(C | A, \neg B), \theta_4 = p(C | A, B)$.}
    \label{fig:pmf_comp}
\end{figure*}


\section{PSDD Multiplication}
To obtain the message in Equation \ref{eq:message} via circuit multiplication, we need to compute
\begin{align}
\label{eq:pc_message}
    \mu_{\mathcal{M}_B \to x_i}(x_i) &= \sum_{\vv \setminus x_i} \pc \left( \mathcal{M}_B(\vv) \right) \cdot \prod_{v_j \in \vv \setminus x_i} \pc\left( p(v_j | \leak) \right)
\end{align}
where $\pc(\cdot)$ denotes the circuit representation of its input and all multiplications denote circuit multiplications. In this case, all PCs are PSDDs.
As discussed in Section \ref{sec:trac_op}, multiplying two compatible circuits takes quadratic time. 
For each message, we perform $(N-1)$ multiplications (where $N$ is the number of bytes in $\vv$). For simplicity, if we assume that each circuit has size $s$, the entire multiplication chain takes at most $O(s^N)$ time, which is exponential in the number of bytes. We emphasize that this is a worst-case bound and note that, depending on the structure of the circuits, the number of operations can be much lower in practice.

% multiplitation still difficult
% 3 ways to approximate: ... 
Unfortunately, this multiplication chain is still intractable for arbitrary posteriors as the circuit grows prohibitively large quickly. However, we do not need to perform \textit{all} of the circuit multiplications, just to marginalize out all but one variable in the resulting circuit. Instead, after performing a single multiplication, we can immediately marginalize out the byte whose distribution we have just multiplied. The rationale behind this approach is the fact that marginalizing out a set of variables from a PSDD results in a \textit{strictly smaller} smooth and decomposable PC (albeit non-deterministic).
\noindent
\makebox[\textwidth]{\parbox{1.1\textwidth}{%
    \begin{align}
    \label{eq:pruned_pc_message}
        \mu_{\mathcal{M}_B \to x_i}(x_i) &= \sum_{v_{N-1}} \dots \left( \sum_{v_2} \left( \sum_{v_1} \pc (\mathcal{M}_B(\vv)) \cdot \pc(p(v_1 | \leak)) \right) \cdot \pc(p(v_2 | \leak)) \right) \dots \cdot \pc(p(v_{N-1} | \leak))
    \end{align}
}}
where $v_1,\dots,v_{N-1} \in \vv \setminus x_i$.
However, since many bits of \textit{different} bytes $v_1,\dots,v_{N-1}$ are heavily entangled in $\pc(\mathcal{M}_B(\vv))$, marginalizing out a single byte does not significantly reduce the size of the circuit. Thus, in practice, this optimization is not enough to tractably compute the entire multiplication chain. 

In essence, there are 3 approaches to this problem that rely on approximation: (1) Approximate the sum $\sum_{\vv \setminus x_i}$ (this is what loopy BP does), (2) approximate $\mathcal{M}_B(\vv)$ (e.g., by removing clauses in the CNF), or (3) approximate the posterior distributions $p(v_j | \leak)$.

% we choose Input Approximations
We choose the third option and investigate methods to perform repeated multiplication tractably by approximating the input distributions.




\section{Input Approximations}

Recall that we wish to compute
\begin{align}
    \mu_{\mathcal{M}_B \to x_i}(x_i) &= \sum_{\vv \setminus x_i} \pc \left( \mathcal{M}_B(\vv) \right) \cdot \prod_{v_j \in \vv \setminus x_i} \pc\left( p(v_j | \leak) \right)
\end{align}
via means of SDD compilation and circuit multiplication. However, in practice, we cannot compute this multiplication chain efficiently: Recall that if we multiply $N$ PSDDs with sizes $s_1, \dots, s_N$, the multiplication chain takes $O(\prod_{i=1}^N s_i)$ operations in general. To alleviate the computational burden, we wish to keep the sizes $s_i$ as small as possible. As discussed previously, one way to achieve this is by means of \emph{approximating} the distributions we compile into PSDDs, such that we do not need as many PSDD nodes to represent the distributions. Next, we discuss two variants of this approach, namely (1) introducing conditional independencies, and (2) sparsifying the distributions before compiling them.

\subsection{Conditional Independence}
% \todo{Write section}
Consider a byte $x$ as a vector of bits $x = (x^{(1)},\dots,x^{(8)})^T \in \mathbb{F}_2^8$ and let $p(x^{(1)},\dots,x^{(8)})$ denote a probability mass function. By the chain rule of probability, we can, for any ordering $x^{(i_1)},\dots,x^{(i_8)}$, write
\begin{align}
    \label{eq:chain_rule}
p\left(x^{(1)},\dots,x^{(8)} \right) &= p \left( x^{(i_1)} \right) p \left( x^{(i_2)} \mid x^{(i_1)} \right) \cdots  p \left( x^{(i_8)} \mid x^{(i_7)}, \dots, x^{(i_1)} \right)
\end{align}
For every byte distribution we compile into a PSDD, we are given the corresponding (projected) vtree $v$ with leaves $x^{(1)},\dots,x^{(8)}$. Assume that traversing $v$ in post-order visits the leaves in the order $x^{(i_1)},\dots,x^{(i_8)}$. The routine that recursively compiles $p$ into a PSDD (shown in Algorithm \ref{alg:pmf_to_psdd}) then decomposes $p$ exactly as shown in Equation \ref{eq:chain_rule}. Since the resulting PSDD makes decisions \emph{based on every bit}, it is a natural idea to decrease the size of the PSDD representation $\pc \left( p(x) \right)$ by \emph{removing decisions}, which corresponds to introducing \emph{conditional independence} assumptions. For example, if we assume that $x^{(i_1)}$ is independent from the rest of the bits, we could write
\begin{align}
p\left(x^{(1)},\dots,x^{(8)} \right) &= p \left( x^{(i_1)} \right) p \left( x^{(i_2)} \right) \cdots  p \left( x^{(i_8)} \mid x^{(i_7)}, \dots, x^{(i_2)} \right)
\end{align}
and thus, never base a decision on $x^{(i_1)}$ in the compiled PSDD. For every bit in $x^{(i_1)},\dots,x^{(i_8)}$, we can either choose to \emph{decide} on that bit, or neglect the decision. Thus, there exist $2^8 = 256$ PSDDs with different conditional independence assumptions, where $255$ of them strictly decrease the size of the PSDD (since we neglect at least one decision).
However, in practice, there are no conditional independencies in the given distributions, and introducing them quickly leads to empirically bad approximations. We will next discuss a different approach that can, empirically, compress the PSDD representations very well, while achieving a good approximation of the input PMFs.

\subsection{Input Sparsification}
\label{sec:sparsification}
When performing a single-trace attack, it is reasonable to assume that at least \textit{some} posterior distributions $p(v_j | \leak), v_j \in \vv$ have \textit{low entropy}, i.e., the probability mass is concentrated on only a few values.\footnote{If we assume the opposite, i.e., that all posterior distributions have high entropy, then it usually becomes unrealistic to perform single-trace attacks in the first place.}

Consequently, we approximate these distributions by \textit{sparsfying} them: We can interpret a PMF $\pp$ as a $d$-dimensional point on the unit simplex, i.e., 
\begin{align}
\pp \in \Delta_{d} \quad \text{with } \Delta_{d} = \left\{ (p_1,\dots,p_d)^T \ | \ \forall i: p_i \leq 0, \|\pp\|_1 = 1 \right\}
\end{align}
where $\|\pp\|_1 = \sum_{i=1}^d |p_i|$ denotes the $L_1$ norm. In our case, we have $d=256$.

For a given distribution $\pp$ and mass threshold $0 < \pi \leq 1$, we produce an approximation $\tilde{\pp}$ such that the maximum number of components $\tilde{p}_i$ are $0$ while $\|\pp\|_1 \geq \pi$. Finally, we renormalize by setting $\tilde{\pp} \leftarrow \tilde{\pp} \oslash \|\tilde{\pp}\|_1$, where $\oslash$ denotes Hadamard (component wise) division. In other words, we clamp the smallest entries in $\pp$ to $0$ while preserving probability mass $\geq \pi$. We call $\tilde{\pp}$ a \textit{$\pi$-sparse} approximation of $\pp$.

Crucially, for an appropriate $\pi$, $\tilde{\pp}$ can represent \textit{modes} of $\pp$ very well and can be succinctly encoded in a PSDD.
Since we need to multiply our \textsc{MixColumn} PSDD with a \textit{product} of posterior distributions $\prod_{v_j \in \vv \setminus x_i} p(v_j | \leak)$, we wish to approximate multiple distributions. 
We will now introduce an information-theoretic upper bound on the approximation error that is introduced by sparsification. 
\begin{theorem}
    \label{theorem:kl}
    Let $p(\vv \mid \leak) = \prod_{j=1}^K p(v_j \mid \leak)$ be a factorized distribution and let $\tilde{p}(\vv \mid \leak)$ be a sparse approximation where $0 \leq k \leq K$ distributions have been sparsified with mass preserving parameter $\pi$. If $\mathcal{J} \subseteq \{1,\dots,K\}$ is the set of indices of sparsified distributions ($|\mathcal{J}| = k$), we can thus write
    \begin{align}
        \tilde{p}(\vv \mid \leak) = \prod_{j \in \tilde{J}} \tilde{p}_\pi(v_j \mid \leak) \cdot \prod_{j \notin \tilde{J}} p(v_j \mid \leak)
    \end{align}
    where $\tilde{p}_\pi$ denotes a $\pi$-sparse approximation of $p$. Then, the (reverse) Kullback-Leibler Divergence $D_{KL}$ between $\tilde{p}(\vv \mid \leak)$ and $p(\vv \mid \leak)$ can be bounded:
    \begin{align}
        D_{KL}\left( \tilde{p}(\vv \mid \leak) \ || \ p(\vv \mid \leak) \right) \leq -\log(\pi) \cdot k
    \end{align}
\end{theorem}
We prove this fact in Appendix \ref{app:kl_proof}.

\section{Most Probable Key Assignment}
\label{sec:mpe}
Until now, we have discussed approaches to obtain marginals over key bytes $p(k_i \mid \leak)$ via BP.
After obtaining these distributions, many attacks in the literature \cite{5min, 32bit} then assume that
\begin{align}
    p(k_1,\dots,k_n \mid \leak) = \prod_{i=1}^n p(k_i \mid \leak)
\end{align}
with $n$ denoting the number of key bytes.
However, this is usually a strong assumption, as in reality, the key bytes might \emph{not} be conditionally independent given the leakages. A thorough analysis of this assumption can be found in Section \label{sec:mar_vs_mpe}.

Moreover, the aim of an attacker is not to compute the full joint distribution, but to learn
\begin{align}
    \argmax_{k_1,\dots,k_n} \ p(k_1,\dots,k_n \mid \leak)
\end{align}
or, in general, the top-$N$ most probable key assignments. Computing (or estimating) marginals from the joint is usually done due to computational advantages, since directly querying the joint distribution is often intractable. 
However, given our problem, we will now describe a method that uses deterministic PCs to tractably\footnote{Assuming that circuit multiplication was tractable in the first place.} compute the most probable key assignment in the resulting posterior distribution.

Recall the factor graph in Figure \ref{fig:aes_acyclic_fg}. The key idea of this approach is to essentially summarize the entire factor graph within a single deterministic PC, which can answer MPE queries tractably. For each $i$, we can, starting from the very left of the factor graph, first send a BP message from $k_i$ to the \textsc{xor} node (as described in Section \ref{sec:bp}), which sends its message to $y_i$. Next, $y_i$ sends its message to the \textsc{sbox} factor node, which passes its message to the $x_i$ node (left-to-right message passing). Finally, $x_i$ collects this message and sends its message $\mu_{x_i \to \mathcal{M}_B}(x_i)$ to the $\mathcal{M}_B$ factor node.
From the perspective of $\mathcal{M}_B$, it receives a message from each $x_i$, as well as from its other neighbours (which are just the posterior distributions of intermediate values).
Recall that $\mathcal{M}_B$ is represented as a PSDD and that we can compile all incoming messages into compatible PSDDs. Instead of computing marginals, we now wish to compute
\begin{align}
\label{eq:pc_argmax}
    \vv^* = \argmax_{\vv} \ \pc \left( \mathcal{M}_B(\vv) \right) \cdot \prod_{v_j \in \vv \setminus \vin} \pc\left( p(v_j | \leak) \right) \cdot \prod_{i=1}^4 \pc \left( \mu_{x_i \to \mathcal{M}_B}(x_i) \right)
\end{align}
Recall that we can represent the entire expression we wish to compute the $\argmax$ of into a single PC by means of circuit multiplication. As the resulting circuit is \emph{still determinsitic}, we can compute the $\argmax$ in linear time in the size of the circuit. Note that $\vv^*$ also contains the most probable assignment for input variables\footnote{This is because given $\vin$, $\vmid$ and $\vout$ follow deterministically.} $\vin$, denoted $x_1^*,\dots,x_4^*$. Finally, we use the deterministic relationship between $x_1^*,\dots,x_4^*$ and the key bytes (given the plaintext bytes) to recover the most probable key assignment $k_1,\dots,k_4$.

Moreover, we can easily generalize this algorithm to find the $N \in \mathbb{N}$ most probable key assignments (top-$N$ MPE query). We discuss the applicability of this approach to general problems in Section \ref{sec:mar_vs_mpe}.
% we can first compute messages from y_i to x_i, producin

\section{Scope of Posterior Distributions}
\label{sec:scopes}

Up to now, we have always assumed that every posterior distribution $p(v | \leak)$ reasons over a single byte, i.e., $v \in \{0,\dots,255\}$. However, in some state-of-the-art side-channel attacks \cite{bit_posterior1, bit_posterior2, breaking_free}, the authors estimate distributions over individual \textit{bits} and assume that $p(v | \leak) = \prod_{i=1}^8 p(v^{(i)} | \leak)$. The message computation in Equation \ref{eq:message} then becomes
\begin{align}
\label{eq:bern_message}
    \mu_{\mathcal{M}_B \to x_i}(x_i) &= \sum_{\vv \setminus x_i} \mathcal{M}_B(\vv) \cdot \prod_{v_j \in \vv \setminus x_i} p(v_j | \leak) \\
    &= \sum_{\vv \setminus x_i} \mathcal{M}_B(\vv) \cdot \prod_{v_j \in \vv \setminus x_i} \prod_{k=1}^8 p \left( v_j^{(k)} | \leak \right)
\end{align}
In other words, the product over byte-distributions reduces to a distribution of fully-factorized Bernoullis. 
\begin{theorem}
\label{thoerem:ff_pc}
Let $\xx = (x_1,\dots,x_n)^T$ be a collection of binary random variables and $p(\xx) = \prod_{i=1}^n p(x_i)$ a product of Bernoulli distributions. For any vtree $v$ over $\xx$, we can construct a PC that encodes $p(\xx)$ and respects $v$ in time $O(n)$. The resulting PC has size $O(n)$ and is smooth, structured decomposable, and deterministic (i.e., the resulting circuit is a PSDD). 
\end{theorem}
A constructive proof of this theorem is given in Appendix \ref{app:ff_pc}.
Hence, we can easily represent the product of Bernoullis as a single PC that is compatible with $\pc(\mathcal{M}_B(\vv))$ --- without explicit circuit multiplications. We can thus compute a message using a single circuit multiplication:
\begin{align}
\label{eq:bern_pc_message}
    \mu_{\mathcal{M}_B \to x_i}(x_i) &= \sum_{\vv \setminus x_i} \pc(\mathcal{M}_B(\vv)) \cdot \pc \left( \prod_{v_j \in \vv \setminus x_i} \prod_{k=1}^8 p \left(v_j^{(k)} | \leak \right) \right)
\end{align}
As shown next, the remaining circuit multiplication is also easy from a computational perspective.
\begin{theorem}
\label{thoerem:ff_pc_mult}
   Assume $p_1$ is a PSDD over $n$ binary random variables that respects vtree $v$ and has size $s_1$, and $p_2$ is a PSDD with size $O(n)$ that encodes a distribution of fully-factorized Bernoullis and was constructed as shown in Appendix \ref{app:ff_pc}. Computing the circuit product $p_1 \cdot p_2$ using Algorithm \ref{alg:psdd_multiplication} takes $O(s_1)$ time and the resulting circuit is of size $O(s_1)$. 
\end{theorem}
A proof of this theorem is given in Appendix \ref{app:ff_pc_mult}.
Consequently, computing belief propagation messages via circuit multiplications is particularly advantageous in the case where the given posterior distributions factorize on the bit level.